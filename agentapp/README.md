**Agentapp — Run Instructions**

- **Project root:**: The agent code lives under the `agentapp` package. Use the `agentapp/requirements.txt` to install dependencies for this component.

**Quick start (Windows PowerShell)**

- **1. Activate environment:**: Use your conda or venv environment. Example with conda:

```
conda activate gpu-env
cd path\to\material-wise\agentapp
```

- **2. Install Python deps:**: (from inside `agentapp`)

```
pip install -r requirements.txt
```

- **3. Run the web app (simple):**: `main.py` starts a Uvicorn server when run directly.

```
python .\api\main.py
```


- **4. Open UI / API:**: Browse to `http://127.0.0.1:8000/` for the web UI. The API endpoint is `POST /api/predict`.

Example `curl` call:

```
curl -X POST http://127.0.0.1:8000/api/predict -H "Content-Type: application/json" -d "{\"product\": \"ppc cement\"}"
```

Environment variables (useful)

- **`SUPPRESS_ACCESS_LOGS`**: Default is `1` (suppress uvicorn access logs). Set to `0` to enable full access logs.
  - PowerShell (session): `$env:SUPPRESS_ACCESS_LOGS='0'`
  - CMD (session): `set SUPPRESS_ACCESS_LOGS=0`
  - Permanent (Windows): `setx SUPPRESS_ACCESS_LOGS 0`

- **LLM backends:**
  - `LLM_BACKEND`: `groq` (default) or `ollama`.
  - `GROQ_API_KEY`: required when using `groq` client.
  - `OLLAMA_URL`: e.g. `http://localhost:11434` when using `ollama` backend.
  - `OLLAMA_MODEL`: model name for Ollama (optional).

Optional crawlers

- The project includes lightweight (requests + BeautifulSoup) crawlers and Selenium-based dynamic crawlers.

- Run the light crawler (no browser required):

```
python .\ingestion\run_crawl_store.py
```

- Run Selenium-based crawlers (requires `selenium`, `webdriver-manager` and a browser):

```
python .\ingestion\run_crawl_selenium.py
python .\ingestion\run_crawl_all_materials.py
python .\ingestion\run_crawl_all_materials_deeper.py
```

Notes for Selenium on Windows

- Install Chrome/Edge/Firefox and ensure the browser binary is discoverable in `PATH` or use the environment variables `<BROWSER>_BINARY` (for example `CHROME_BINARY`).
- The code uses `webdriver-manager` to download drivers automatically (ChromeDriverManager, GeckoDriverManager, EdgeChromiumDriverManager).

Models & data

- `data/price_index.csv` is used by the feature builder; ensure this file exists and has the expected columns (`comm_name`, `comm_code`, `comm_wt`, and `indxMMYYYY` columns).
- Optional trained model files (if you have them): place `trend_model.pkl` and `model_features.pkl` under `models/` (the code will fallback to a deterministic rule if models are missing).

Troubleshooting

- If you still see many access log lines, confirm `SUPPRESS_ACCESS_LOGS` is set before starting the server and that you are running `python .\api\main.py` (the script controls `uvicorn.run(..., access_log=...)`).
- If `bs4` or `lxml` errors occur, install: `pip install beautifulsoup4 lxml`.
- For missing model errors: the server will use a safe deterministic fallback; to remove the fallback, add your trained model files to `models/`.

Advanced / Development

- Run tests or run individual modules from the `agentapp` package using `python -m agentapp.ingestion.crawler` style (run from repository root or ensure `agentapp` is on `PYTHONPATH`).

Contact / Next steps

- If you want pinned dependency versions, I can generate a lockfile or pin `agentapp/requirements.txt` from your current environment.

---
Generated by the project maintainer helper. Adjust paths and env var examples to match your environment.
# AgentApp — Material Wise MCP Agent

This `agentapp` package provides a separate, modular implementation of the Material Cost Prediction (MCP) Agent.

Key features
- Multiple scrapers (BuildersMART, IndiaMART) that extract numeric prices only
- Feature engineering from existing `data/price_index.csv`
- Prediction engine that loads a trained model from `models/` if available; deterministic fallback otherwise
- Climate risk via Open-Meteo (reuses `services.climate`)
- Groq reasoning wrapper (uses `GROQ_API_KEY` if configured; otherwise returns deterministic, evidence-based output and documents missing LLM)
- FastAPI JSON API and minimal web UI at `/`

Run
----
Install dependencies:

```bash
pip install -r agentapp/requirements.txt
```

Start the app (from project root):

```bash
python -m agentapp.api.main
```

Open http://127.0.0.1:8000

Notes & limitations
- Scrapers attempt to extract server-rendered numeric prices only. Many marketplaces use JS and may not expose prices server-side; when that happens scrapers will mark data as unavailable and explain why.
- The Groq LLM is used only when `GROQ_API_KEY` is set; otherwise the system returns a deterministic, auditable reasoning text that does not invent prices.
- The prediction engine prefers a trained model located at `models/trend_model.pkl`. If missing, a transparent fallback rule is used (based on recent price change).

Ethics & transparency
- No fabricated prices — when live prices are missing the system states why and marks market as unavailable.
- All sources are cited with clickable links in responses.
- LLM outputs are constrained to evidence and the system documents when the LLM is not available.
